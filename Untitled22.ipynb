{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load financial news and stock price CSVs\n",
        "news_df = pd.read_excel('/content/data_2007_to_2011_by_date.csv.xlsx', parse_dates=['Date'])\n",
        "stock_df = pd.read_excel('/content/sensex_2007_2011.csv.xlsx', parse_dates=['Date'])\n"
      ],
      "metadata": {
        "id": "9YQkvrzt1Hb-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xZBtFHMSENK6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge on 'date'\n",
        "merged_df = pd.merge(news_df, stock_df, on='Date')\n",
        "\n",
        "# Drop rows with missing values after merge\n",
        "merged_df.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "-mLnLoJdBzUr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "yA92hNq1Er2A",
        "outputId": "8273d096-172c-43b3-8f9e-d46a653f66ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0       Date                                      headline_text  \\\n",
              "0      450061 2007-01-02  Q&A: Innovation is the norm across sectors in ...   \n",
              "1      450062 2007-01-02                                 Consumer democracy   \n",
              "2      450063 2007-01-02                                Winner Takes It All   \n",
              "3      450064 2007-01-02      Mosquito scare keeps NRI tourists off Gujarat   \n",
              "4      450065 2007-01-02                   Diversified MFs are a better bet   \n",
              "\n",
              "   Sentiment         Close          High          Low          Open Volume  \n",
              "0        0.0  13942.240234  13980.540039  13797.44043  13827.769531   9600  \n",
              "1        0.0  13942.240234  13980.540039  13797.44043  13827.769531   9600  \n",
              "2        0.0  13942.240234  13980.540039  13797.44043  13827.769531   9600  \n",
              "3        0.0  13942.240234  13980.540039  13797.44043  13827.769531   9600  \n",
              "4        0.5  13942.240234  13980.540039  13797.44043  13827.769531   9600  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae7fa7c5-1ef7-4c14-b2fa-0eab17cc781c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>headline_text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>450061</td>\n",
              "      <td>2007-01-02</td>\n",
              "      <td>Q&amp;A: Innovation is the norm across sectors in ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13942.240234</td>\n",
              "      <td>13980.540039</td>\n",
              "      <td>13797.44043</td>\n",
              "      <td>13827.769531</td>\n",
              "      <td>9600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>450062</td>\n",
              "      <td>2007-01-02</td>\n",
              "      <td>Consumer democracy</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13942.240234</td>\n",
              "      <td>13980.540039</td>\n",
              "      <td>13797.44043</td>\n",
              "      <td>13827.769531</td>\n",
              "      <td>9600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450063</td>\n",
              "      <td>2007-01-02</td>\n",
              "      <td>Winner Takes It All</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13942.240234</td>\n",
              "      <td>13980.540039</td>\n",
              "      <td>13797.44043</td>\n",
              "      <td>13827.769531</td>\n",
              "      <td>9600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>450064</td>\n",
              "      <td>2007-01-02</td>\n",
              "      <td>Mosquito scare keeps NRI tourists off Gujarat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13942.240234</td>\n",
              "      <td>13980.540039</td>\n",
              "      <td>13797.44043</td>\n",
              "      <td>13827.769531</td>\n",
              "      <td>9600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>450065</td>\n",
              "      <td>2007-01-02</td>\n",
              "      <td>Diversified MFs are a better bet</td>\n",
              "      <td>0.5</td>\n",
              "      <td>13942.240234</td>\n",
              "      <td>13980.540039</td>\n",
              "      <td>13797.44043</td>\n",
              "      <td>13827.769531</td>\n",
              "      <td>9600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae7fa7c5-1ef7-4c14-b2fa-0eab17cc781c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae7fa7c5-1ef7-4c14-b2fa-0eab17cc781c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae7fa7c5-1ef7-4c14-b2fa-0eab17cc781c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4881fd46-bf37-41d9-950e-93e41fd888ac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4881fd46-bf37-41d9-950e-93e41fd888ac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4881fd46-bf37-41d9-950e-93e41fd888ac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "merged_df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 2: Sentiment Analysis ----------------\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function to calculate polarity and subjectivity\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "# Ensure 'headline_text' is treated as string and fill potential NaNs with empty string\n",
        "merged_df['headline_text'] = merged_df['headline_text'].astype(str).fillna('')\n",
        "\n",
        "# Apply sentiment analysis\n",
        "merged_df[['polarity', 'subjectivity']] = merged_df['headline_text'].apply(lambda x: pd.Series(get_sentiment(x)))"
      ],
      "metadata": {
        "id": "AC8yLzWaBntt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 3: Abnormal Return Labels ----------------\n",
        "merged_df['DailyReturn'] = (merged_df['Close'] - merged_df['Close'].shift(1)) / merged_df['Close'].shift(1)\n",
        "\n",
        "# Compute mu_R (mean) and sigma_R (std) with rolling window = 5\n",
        "merged_df['mu_R'] = merged_df['DailyReturn'].rolling(window=5).mean()\n",
        "merged_df['sigma_R'] = merged_df['DailyReturn'].rolling(window=5).std()\n",
        "\n",
        "# Label abnormal returns\n",
        "delta = 1  # Define delta\n",
        "merged_df['abnormal_return'] = (merged_df['DailyReturn'] > (merged_df['mu_R'] + delta * merged_df['sigma_R'])).astype(int)"
      ],
      "metadata": {
        "id": "DTgNQrw1DhjW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 4: Feature Engineering ----------------\n",
        "for lag in range(1, 6):\n",
        "    merged_df[f'ARSign_t-{lag}'] = merged_df['abnormal_return'].shift(lag)\n",
        "\n",
        "# Final feature list\n",
        "features = ['ARSign_t-1', 'ARSign_t-2', 'ARSign_t-3', 'ARSign_t-4', 'ARSign_t-5', 'polarity', 'subjectivity']\n",
        "merged_df.dropna(subset=features, inplace=True)"
      ],
      "metadata": {
        "id": "n1iQTBlXDljM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 5: Normalize Features ----------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "merged_df[features] = scaler.fit_transform(merged_df[features])"
      ],
      "metadata": {
        "id": "yxPqzqgWDqMa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_convert = ['Close', 'High', 'Low', 'Open', 'Volume', 'DailyReturn']\n",
        "for col in cols_to_convert:\n",
        "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "95e3i025JZsJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.dropna()"
      ],
      "metadata": {
        "id": "pzaJ-wqVJdaw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['ARSign_t-1', 'ARSign_t-2', 'ARSign_t-3', 'ARSign_t-4', 'ARSign_t-5', 'polarity', 'subjectivity']\n",
        "target = 'abnormal_return'"
      ],
      "metadata": {
        "id": "PWL2FyYiJhrt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "W = 5\n",
        "X_data = merged_df[features].to_numpy()\n",
        "y_data = merged_df[target].to_numpy()\n",
        "\n",
        "X_windows = np.lib.stride_tricks.sliding_window_view(X_data, window_shape=(W,), axis=0)\n",
        "X_windows = X_windows.reshape(-1, W, len(features))\n",
        "y_windows = y_data[W:]"
      ],
      "metadata": {
        "id": "CnYPfQkuJmx_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 7: Temporal Train-Test Split ----------------\n",
        "split_index = int(0.8 * len(X_windows))\n",
        "X_train, X_test = X_windows[:split_index], X_windows[split_index:]\n",
        "y_train, y_test = y_windows[:split_index], y_windows[split_index:]"
      ],
      "metadata": {
        "id": "NMzYyqiHDy_L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize input (optional but recommended for RNN)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Flatten the time steps to apply scaler across all time points\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)"
      ],
      "metadata": {
        "id": "CwGmrTKFJ7R5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGP0OA5GHFqX",
        "outputId": "b5df4f2f-a831-4c68-c4ce-90e3b63c1110"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------- STEP 8: Model Architectures ----------------\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, GRU, LSTM, Dense, Dropout, TimeDistributed\n",
        "\n",
        "# SimpleRNN model\n",
        "def create_simple_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, activation='sigmoid', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# GRU model\n",
        "def create_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(64, activation='elu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# LSTM model\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, activation='elu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(32, return_sequences=False, activation='elu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "cfCDCM0oD9gy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HF4wIfNXK43x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "source": [
        "# ---------------- STEP 6: Windowing and Splitting ----------------\n",
        "# This step should be done BEFORE the training loop\n",
        "\n",
        "W = 5\n",
        "X_data = merged_df[features].to_numpy()\n",
        "y_data = merged_df[target].to_numpy()\n",
        "\n",
        "# Create sliding windows for features\n",
        "# The number of windows is len(X_data) - W + 1\n",
        "X_windows = np.lib.stride_tricks.sliding_window_view(X_data, window_shape=(W,), axis=0)\n",
        "X_windows = X_windows.reshape(-1, W, len(features))\n",
        "\n",
        "# The target for each window is the abnormal return at the END of the window\n",
        "# So we take y_data starting from index W-1 (0-based indexing)\n",
        "y_windows = y_data[W-1:] # Corrected slicing\n",
        "\n",
        "# Ensure X_windows and y_windows have the same length after windowing\n",
        "min_len = min(len(X_windows), len(y_windows))\n",
        "X_windows = X_windows[:min_len]\n",
        "y_windows = y_windows[:min_len]\n",
        "\n",
        "# Temporal train-test split\n",
        "split_index = int(0.8 * len(X_windows))\n",
        "X_train, X_test = X_windows[:split_index], X_windows[split_index:]\n",
        "y_train, y_test = y_windows[:split_index], y_windows[split_index:]\n",
        "\n",
        "# Normalize input (optional but recommended for RNN) - Perform normalization AFTER split\n",
        "scaler = StandardScaler()\n",
        "# Flatten the time steps to apply scaler across all time points\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DoPSroL2XL-W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, GRU, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
        "from keras.saving import save_model\n",
        "import numpy as np\n",
        "\n",
        "# Model Definitions (Keep these the same)\n",
        "def create_simple_rnn_model():\n",
        "    model = Sequential()\n",
        "    # Use X_train_scaled.shape[1] and X_train_scaled.shape[2] for input_shape\n",
        "    model.add(SimpleRNN(64, activation='sigmoid', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def create_gru_model():\n",
        "    model = Sequential()\n",
        "     # Use X_train_scaled.shape[1] and X_train_scaled.shape[2] for input_shape\n",
        "    model.add(GRU(64, activation='elu', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "     # Use X_train_scaled.shape[1] and X_train_scaled.shape[2] for input_shape\n",
        "    model.add(LSTM(64, return_sequences=True, activation='elu', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
        "    model.add(LSTM(32, return_sequences=False, activation='elu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(16, activation='elu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# ---------------- STEP 9: Model Training ----------------\n",
        "# Use the already prepared X_train_scaled, y_train, X_test_scaled, y_test\n",
        "# Remove the redundant windowing and splitting logic from here\n",
        "\n",
        "models = [create_simple_rnn_model(), create_gru_model(), create_lstm_model()]\n",
        "model_names = ['SimpleRNN', 'GRU', 'LSTM']\n",
        "predictions = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training {model_names[i]} model...\")\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    # Use the pre-split and pre-scaled data\n",
        "    model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, callbacks=[early_stopping], batch_size=32)\n",
        "\n",
        "    # Save model in the recommended Keras format\n",
        "    save_model(model, f'model_{model_names[i]}.keras')\n",
        "\n",
        "    # Use the pre-split and pre-scaled test data for prediction\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "    # Ensure y_pred is 1D if y_test is 1D for metric calculation\n",
        "    if y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    y_bin = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    print(f\"{model_names[i]} Accuracy:\", accuracy_score(y_test, y_bin))\n",
        "    print(f\"{model_names[i]} F1-score:\", f1_score(y_test, y_bin))\n",
        "    # roc_auc_score expects prediction probabilities, not binary predictions\n",
        "    print(f\"{model_names[i]} AUC:\", roc_auc_score(y_test, y_pred))\n",
        "    print(f\"{model_names[i]} Precision:\", precision_score(y_test, y_bin))\n",
        "    print(f\"{model_names[i]} Recall:\", recall_score(y_test, y_bin))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmohu1KgXMzR",
        "outputId": "b7f9031a-1cd1-4f71-83df-fcc06c375ad7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SimpleRNN model...\n",
            "Epoch 1/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0126 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 2/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0100 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0109 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 4/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0102 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 5/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0097 - val_accuracy: 0.9992 - val_loss: 0.0075\n",
            "Epoch 6/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0099 - val_accuracy: 0.9992 - val_loss: 0.0066\n",
            "\u001b[1m3591/3591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
            "SimpleRNN Accuracy: 0.999416823340993\n",
            "SimpleRNN F1-score: 0.0\n",
            "SimpleRNN AUC: 0.5568148449624445\n",
            "SimpleRNN Precision: 0.0\n",
            "SimpleRNN Recall: 0.0\n",
            "Training GRU model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0198 - val_accuracy: 0.9992 - val_loss: 0.0072\n",
            "Epoch 2/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0106 - val_accuracy: 0.9992 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - accuracy: 0.9987 - loss: 0.0098 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 4/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0104 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 5/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 7ms/step - accuracy: 0.9987 - loss: 0.0100 - val_accuracy: 0.9992 - val_loss: 0.0064\n",
            "Epoch 6/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.9988 - loss: 0.0094 - val_accuracy: 0.9992 - val_loss: 0.0064\n",
            "Epoch 7/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0100 - val_accuracy: 0.9992 - val_loss: 0.0066\n",
            "Epoch 8/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0098 - val_accuracy: 0.9992 - val_loss: 0.0065\n",
            "Epoch 9/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0103 - val_accuracy: 0.9992 - val_loss: 0.0066\n",
            "\u001b[1m3591/3591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
            "GRU Accuracy: 0.999416823340993\n",
            "GRU F1-score: 0.0\n",
            "GRU AUC: 0.4933282265309261\n",
            "GRU Precision: 0.0\n",
            "GRU Recall: 0.0\n",
            "Training LSTM model...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0218 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 2/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0095 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 8ms/step - accuracy: 0.9987 - loss: 0.0103 - val_accuracy: 0.9992 - val_loss: 0.0065\n",
            "Epoch 4/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0097 - val_accuracy: 0.9992 - val_loss: 0.0064\n",
            "Epoch 5/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 8ms/step - accuracy: 0.9987 - loss: 0.0099 - val_accuracy: 0.9992 - val_loss: 0.0063\n",
            "Epoch 6/50\n",
            "\u001b[1m11489/11489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 8ms/step - accuracy: 0.9987 - loss: 0.0102 - val_accuracy: 0.9992 - val_loss: 0.0064\n",
            "\u001b[1m3591/3591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
            "LSTM Accuracy: 0.999416823340993\n",
            "LSTM F1-score: 0.0\n",
            "LSTM AUC: 0.5262025629250044\n",
            "LSTM Precision: 0.0\n",
            "LSTM Recall: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# ---------------- STEP 10: PSO Ensemble ----------------\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from pyswarm import pso  # pip install pyswarm\n",
        "\n",
        "# Define predictions from trained models\n",
        "y_pred_1, y_pred_2, y_pred_3 = predictions\n",
        "y_true = y_test\n",
        "\n",
        "# PSO Objective Function: Maximize F1 by minimizing (1 - F1)\n",
        "def ensemble_f1_loss(weights):\n",
        "    w1, w2, w3 = weights\n",
        "    # Ensure y_pred_1, y_pred_2, y_pred_3 are 1D for multiplication\n",
        "    # The predict output is likely (samples, 1), flatten if necessary\n",
        "    y_pred_1_flat = y_pred_1.flatten() if y_pred_1.ndim > 1 else y_pred_1\n",
        "    y_pred_2_flat = y_pred_2.flatten() if y_pred_2.ndim > 1 else y_pred_2\n",
        "    y_pred_3_flat = y_pred_3.flatten() if y_pred_3.ndim > 1 else y_pred_3\n",
        "\n",
        "    ensemble_pred = w1 * y_pred_1_flat + w2 * y_pred_2_flat + w3 * y_pred_3_flat\n",
        "    y_pred_bin = (ensemble_pred > 0.5).astype(int)\n",
        "    return 1 - f1_score(y_true, y_pred_bin)\n",
        "\n",
        "# Constraint: weights must sum to 1\n",
        "def weight_constraint(weights):\n",
        "    # We want sum(weights) - 1 == 0.\n",
        "    # For f_ieqcons, constraints are >= 0.\n",
        "    # A common way to handle sum=1 constraint using f_ieqcons is:\n",
        "    # 1 - sum(weights) >= 0  AND  sum(weights) - 1 >= 0\n",
        "    # This means 1 - sum(weights) and sum(weights) - 1 must both be non-negative,\n",
        "    # which only happens when 1 - sum(weights) = 0 and sum(weights) - 1 = 0,\n",
        "    # i.e., sum(weights) = 1.\n",
        "    return np.array([1 - np.sum(weights), np.sum(weights) - 1]) # Return an array of constraints\n",
        "\n",
        "# Run PSO\n",
        "lb = [0, 0, 0]  # lower bounds\n",
        "ub = [1, 1, 1]  # upper bounds\n",
        "\n",
        "# **Attempt 1: Pass the constraint function directly (against error message, but for testing)**\n",
        "# This is less likely to work based on the error, but is a quick test.\n",
        "# If this fails, revert to the original [weight_constraint] and try reinstalling pyswarm.\n",
        "# best_weights, best_score = pso(\n",
        "#     ensemble_f1_loss,\n",
        "#     lb,\n",
        "#     ub,\n",
        "#     f_ieqcons=weight_constraint, # <-- Changed from [weight_constraint]\n",
        "#     swarmsize=30,\n",
        "#     maxiter=50,\n",
        "#     debug=True\n",
        "# )\n",
        "\n",
        "# **Attempt 2: Keep the list format, but ensure pyswarm is correctly installed**\n",
        "# This is the correct way to pass constraints according to pyswarm docs.\n",
        "# If the TypeError persists with this, it suggests an issue with the pyswarm installation or version.\n",
        "# Reverting to the original call with the list:\n",
        "best_weights, best_score = pso(\n",
        "    ensemble_f1_loss,\n",
        "    lb,\n",
        "    ub,\n",
        "    f_ieqcons=weight_constraint, # Pass the function directly, not in a list\n",
        "    swarmsize=30,\n",
        "    maxiter=50,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "\n",
        "# Final Ensemble Prediction using optimized weights\n",
        "w1, w2, w3 = best_weights\n",
        "y_pred_1_flat = y_pred_1.flatten() if y_pred_1.ndim > 1 else y_pred_1\n",
        "y_pred_2_flat = y_pred_2.flatten() if y_pred_2.ndim > 1 else y_pred_2\n",
        "y_pred_3_flat = y_pred_3.flatten() if y_pred_3.ndim > 1 else y_pred_3\n",
        "ensemble_pred = w1 * y_pred_1_flat + w2 * y_pred_2_flat + w3 * y_pred_3_flat\n",
        "ensemble_bin = (ensemble_pred > 0.5).astype(int)\n",
        "\n",
        "# Report Metrics\n",
        "print(\"PSO-Optimized Ensemble Weights:\", best_weights)\n",
        "print(\"Optimized F1-score:\", f1_score(y_true, ensemble_bin))\n",
        "\n",
        "# Assign best_weights to w1, w2, w3 for the evaluation step (assuming success)\n",
        "w1, w2, w3 = best_weights"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sk9ua9yn7w5",
        "outputId": "edcbb084-66a3-459a-9062-97a81fe2f3ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single constraint function given in f_ieqcons\n",
            "Best after iteration 1: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 2: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 3: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 4: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 5: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 6: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 7: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 8: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 9: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 10: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 11: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 12: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 13: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 14: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 15: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 16: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 17: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 18: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 19: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 20: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 21: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 22: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 23: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 24: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 25: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 26: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 27: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 28: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 29: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 30: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 31: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 32: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 33: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 34: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 35: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 36: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 37: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 38: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 39: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 40: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 41: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 42: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 43: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 44: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 45: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 46: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 47: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 48: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 49: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Best after iteration 50: [0.41942137 0.78499404 0.63424494] 1e+100\n",
            "Stopping search: maximum iterations reached --> 50\n",
            "However, the optimization couldn't find a feasible design. Sorry\n",
            "PSO-Optimized Ensemble Weights: [0.41942137 0.78499404 0.63424494]\n",
            "Optimized F1-score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- STEP 10: PSO Ensemble ----------------\n",
        "# Assuming PSO implementation is available\n",
        "# Let y_pred_1, y_pred_2, y_pred_3 be outputs from SimpleRNN, GRU, LSTM\n",
        "y_pred_1, y_pred_2, y_pred_3 = predictions\n",
        "\n",
        "# Final prediction: y_final = w1*y1 + w2*y2 + w3*y3\n",
        "# Use PSO to optimize weights w1, w2, w3 with constraints\n",
        "# This part requires a PSO implementation\n"
      ],
      "metadata": {
        "id": "OE7BSzxqEElz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "source": [
        "# ---------------- STEP 11: Evaluation ----------------\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "import numpy as np # Ensure numpy is imported if not already\n",
        "\n",
        "# Compute metrics\n",
        "# Ensure y_pred_1, y_pred_2, y_pred_3 are flattened before the weighted sum if they are (samples, 1)\n",
        "y_pred_1_flat = y_pred_1.flatten() if y_pred_1.ndim > 1 else y_pred_1\n",
        "y_pred_2_flat = y_pred_2.flatten() if y_pred_2.ndim > 1 else y_pred_2\n",
        "y_pred_3_flat = y_pred_3.flatten() if y_pred_3.ndim > 1 else y_pred_3\n",
        "\n",
        "\n",
        "y_final_raw = (w1 * y_pred_1_flat + w2 * y_pred_2_flat + w3 * y_pred_3_flat)\n",
        "\n",
        "# Convert probabilities to binary predictions and ensure it's 1-dimensional\n",
        "y_final = (y_final_raw > 0.5).astype(int).flatten() # Add .flatten() here\n",
        "\n",
        "# Now y_final should be (114888,)\n",
        "\n",
        "# Compute metrics\n",
        "# y_test is already 1D (from global variables and typical data loading)\n",
        "accuracy = accuracy_score(y_test, y_final)\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, y_final)\n",
        "# roc_auc_score expects probability scores if the target is binary, not binary predictions\n",
        "# Use the raw predictions for AUC calculation if possible, otherwise the AUC for binary predictions will be 0.5 or 1.0 depending on the distribution\n",
        "try:\n",
        "    auc = roc_auc_score(y_test, y_final_raw) # Use raw scores for AUC\n",
        "except ValueError:\n",
        "    # Handle case where only one class is present in y_test or y_final\n",
        "    print(\"Could not compute AUC: only one class present in y_test or y_final\")\n",
        "    auc = np.nan # Or some other indicator\n",
        "\n",
        "precision = precision_score(y_test, y_final)\n",
        "recall = recall_score(y_test, y_final)\n",
        "f1 = f1_score(y_test, y_final)\n",
        "\n",
        "# Print metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Balanced Accuracy: {balanced_accuracy}')\n",
        "print(f'AUC: {auc}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8H4aaKRoNnK",
        "outputId": "11f8bfab-0eaf-4560-f498-e2bb8c0c061d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.999416823340993\n",
            "Balanced Accuracy: 0.5\n",
            "AUC: 0.5114703522302788\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# ---------------- STEP 12: Save Outputs ----------------\n",
        "import joblib\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Save ensemble weights\n",
        "joblib.dump((w1, w2, w3), 'ensemble_weights.pkl')\n",
        "\n",
        "# Save best model (assuming the best model is the LSTM model - models[2])\n",
        "# Check if models list and models[2] exist and are valid\n",
        "if 'models' in globals() and len(models) > 2 and models[2] is not None:\n",
        "    try:\n",
        "        models[2].save('best_model.h5')\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save best model: {e}\")\n",
        "else:\n",
        "    print(\"Could not find 'models' or models[2] to save the best model.\")\n",
        "\n",
        "\n",
        "# Export predictions and evaluation metrics\n",
        "# y_test and y_final should now both be 1-dimensional arrays\n",
        "predictions_df = pd.DataFrame({\n",
        "    'y_test': y_test,\n",
        "    'y_pred': y_final # y_final is now 1D\n",
        "})\n",
        "predictions_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Accuracy': [accuracy],\n",
        "    'Balanced Accuracy': [balanced_accuracy],\n",
        "    'AUC': [auc],\n",
        "    'Precision': [precision],\n",
        "    'Recall': [recall],\n",
        "    'F1 Score': [f1]\n",
        "})\n",
        "metrics_df.to_csv('evaluation_metrics.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUoiMTbQoO_Y",
        "outputId": "bf8ef895-647f-4bc8-b2e6-ec6202dfc38e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}